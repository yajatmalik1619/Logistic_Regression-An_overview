# Logistic Regression â€” Concept & Example

---

## ğŸ”¹ Introduction

**Goal:** Model the probability that an input belongs to a certain class (e.g., benign vs malignant tumor).  
Unlike linear regression, logistic regression predicts probabilities constrained between 0 and 1.

---

## âš™ï¸ The Logistic Function

The logistic (sigmoid) function converts any real value into a probability:

sigma(z) = e^z/(1+e^z)
OR
Ïƒ(z) = e^z/(1+e^z)

where z=wâŠ¤x+b
**Interpretation:**  
As zâ†’+âˆ , Ïƒ(z)â†’1
As zâ†’âˆ’âˆ , Ïƒ(z)â†’0

**Slide image:**  
ğŸ“Š *Sigmoid Function â€” `assets/sigmoid.png`*

---

## ğŸ”¹ Decision Boundary Intuition

A linear model separates the input space using a decision boundary where \( P(y=1|x)=0.5 \).  

- Samples on one side â†’ predicted 1 (positive class)  
- Samples on the other â†’ predicted 0 (negative class)

**Slide image:**  
ğŸ¨ *Decision Boundary â€” `assets/decision_boundary.png`*

---

## ğŸ“ Log-Loss Function (Cross Entropy)

To train the model, we minimize **log-loss**:

â„“(w)=âˆ’N1â€‹i=1âˆ‘Nâ€‹[yiâ€‹log(y^â€‹iâ€‹)+(1âˆ’yiâ€‹)log(1âˆ’y^â€‹iâ€‹)]

where 
y^â€‹iâ€‹=Ïƒ(wâŠ¤xiâ€‹)=1+eâˆ’(wâŠ¤xiâ€‹)1â€‹

- Penalizes confident wrong predictions heavily  
- Encourages probabilities to match true labels

**Slide image:**  
ğŸ“‰ *Logistic Loss for y=0 and y=1 â€” `assets/logistic_loss.png`*

---

## ğŸ§  Gradient Descent Update Rule

We update model weights by moving opposite to the gradient:

wâ†wâˆ’Î·âˆ‡wL
where Î· is the learning rate controlling the step size.

The gradient of the loss is:

âˆ‡wâ€‹L=N1â€‹i=1âˆ‘Nâ€‹(Ïƒ(ziâ€‹)âˆ’yiâ€‹)xiâ€‹
---

## ğŸ” Interpreting Coefficients

Each coefficient wj represents the influence of feature xj on the log-odds:

logP(y=0âˆ£x)P(y=1âˆ£x)â€‹=wâŠ¤x+b

- Positive wj : increases likelihood of class 1  
- Negative wj : decreases likelihood of class 1  

**Slide image:**  
ğŸ“Š *Top Feature Coefficients â€” `assets/coefficients.png`*

---

## ğŸ§¬ Application Example â€” Breast Cancer Prediction

Dataset: *Breast Cancer Wisconsin (Diagnostic)*  
- 30 features describing cell nuclei  
- Target: 0 = Malignant, 1 = Benign

Model: Logistic Regression (with StandardScaler & train/test split)  

**Performance Highlights:**
| Metric | Value |
|:--|--:|
| Accuracy | ~97% |
| ROC-AUC | ~0.99 |
| Precision | ~0.97 |
| Recall | ~0.97 |

**Slide image suggestions:**  
- ROC curve (from app)  
- Confusion matrix (from app)

---

## ğŸ’¡ Key Takeaways

- Logistic regression is **interpretable** and **efficient** for binary classification.  
- Outputs **probabilities**, not just class labels.  
- Great baseline model before trying more complex algorithms.  
- Coefficients reveal **feature importance** in decision making.

---

## ğŸ“š Recommended Reading

- *Scikit-learn Documentation* â€” Logistic Regression User Guide  
- *Andrew Ng* â€” Machine Learning Lecture Notes (Coursera)  
- *The Elements of Statistical Learning* â€” Chapter on Generalized Linear Models  
- *Sebastian Raschka* â€” Logistic Regression Explained (blog post)  

---

## ğŸ Conclusion

The logistic regression model provides a strong, interpretable foundation for medical diagnosis problems.  
By mapping continuous input data into probability space using the sigmoid function,  
it effectively distinguishes between malignant and benign tumors.

